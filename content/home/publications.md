---
# An instance of the Pages widget.
# Documentation: https://wowchemy.com/docs/page-builder/
widget: pages

url: #publications

# This file represents a page section.
headless: true

# Order that this section appears on the page.
weight: 25

title: Publications
subtitle: ''

content:
  # Page type to display. E.g. post, talk, publication...
  page_type: publication
  # Choose how much pages you would like to display (0 = all pages)
  count: 0
  # Choose how many pages you would like to offset by
  offset: 0
  # Page order: descending (desc) or ascending (asc) date.
  order: desc
  # Filter on criteria
  filters:
    tag: ''
    category: ''
    publication_type: ''
    author: ''
    exclude_featured: true
design:
  # Choose a view for the listings:
  #   1 = List
  #   2 = Compact
  #   3 = Card
  #   4 = Citation (publication only)
  view: 1
---

### Technology

[Oct 2021] [Safety Harness](https://docs.cohere.ai/safety-harness): I led the design and implementation of Cohere's (work-in-progress) language model safety evaluation system, and wrote the linked documentation to share insights with our developers.

[Sep 2021] [Mitigating harm in language models with conditional-likelihood filtration](https://arxiv.org/abs/2108.07790): this paper describes how the safety team at Cohere filters nuanced harmful text from huge training corpora using large pre-trained language models. Project led by [Helen Ngo](https://www.mathemakitten.dev/).

[Dec 2020] [Short-Term Solar Irradiance Forecasting Using Calibrated Probabilistic Models](https://www.climatechange.ai/papers/neurips2020/6.html): I was fortunate to participate in the [AI for Climate Change](https://stanfordmlgroup.github.io/programs/aicc-bootcamp/) research bootcamp at Stanford and contribute to the early stages of the linked research. Project led by [Eric Zelikman](https://zelikman.me/).

### Society

[July 2021] [Assessing the safety risks of software written by artificial intelligence](https://techpolicy.press/assessing-the-safety-risks-of-software-written-by-artificial-intelligence/): A piece investigating the risk landscape around the application of large language models to code generation. I focus on OpenAI's [Codex](https://openai.com/blog/openai-codex/) model as a case study, as it was recently deployed to thousands of developers with [Github Copilot](https://copilot.github.com/). Published to [Tech Policy Press](https://techpolicy.press/) and edited by [Justin Hendrix](https://www.linkedin.com/in/justinhendrix/).

[May 2021] [Assessing the risks of language model “deepfakes” to democracy](https://techpolicy.press/assessing-the-risks-of-language-model-deepfakes-to-democracy/): A piece containing my thoughts on why we didn't see substantial use of text deepfakes during the 2020 US election and how risks may evolve alongside innovations in language modeling tech. Also published to [Tech Policy Press](https://techpolicy.press/).

[April 2021] [Cohere Responsible Use Documentation](https://docs.cohere.ai/responsible-use): I was a major contributor to Cohere's responsible use documentation, a series of webpages that outline usage guidelines, communicate the limitations and biases of our language models, host the Cohere model cards and data statements, and provide other essential information to Cohere Platform developers.

[Oct 2020] [Then and Now: How Online Conversations About Voter Fraud Have Changed Since 2016](https://www.eipartnership.net/rapid-response/2016-vs-2020): A brief investigation into how narratives about voter fraud were different during the 2016 and 2020 US presidential election. This was the second blog post I contributed to while an analyst with the [Election Integrity Partnership](https://www.eipartnership.net); I was fortunate to collaborate with [Renee Diresta](https://fsi.stanford.edu/people/renee-diresta), [Ben Nimmo](https://www.atlanticcouncil.org/expert/ben-nimmo/), and other researchers from the partnership on this particular post.

[Oct 2020] [Seeking To Help and Doing Harm: The Case of Well-Intentioned Misinformation](): A study about the prevalence of misinformation stemming from well-intentioned actors during the 2020 US presidential election. This was the first blog post I contributed to while an analyst with the [Election Integrity Partnership](https://www.eipartnership.net); I was fortunate to collaborate with [Emerson T. Brooking](https://www.atlanticcouncil.org/expert/emerson-t-brooking/) and other researchers from the partnership on this particular post.

[July 2020] [Updating the Human Priors](https://medium.com/@nrmarda/updating-the-human-priors-3303d9b7133e): Takeaways from teaching CS82SI Workshops for Wellness in Tech at Stanford during Spring 2020. In collaboration with the fantastic [Nik Marda](https://nikmarda.com/) and [Sonia Garcia](https://www.linkedin.com/in/soniapgarcia/).

### Creative

[Jan 2021] [Reanimating the Digital Zombie](https://stanfordrewired.com/post/digital-zombie): A fun collaboration with writer and friend [Jake Zawlacki](https://creees.stanford.edu/news/jake-zawlacki-publishes-capstone-research-folklorica-journal) - this piece stemmed from a late-night conversation about zombie theory, the phrase "digital zombies", and our frustration with attention-grabbing media. Huge shoutouts to our editor Irene Han, [Jason Zhao](https://twitter.com/jasonjzhao?lang=en) for helping to found the unbelievably cool [Rewired](https://stanfordrewired.com/) magazine at Stanford, and my friend [Rayan Krishnan](https://www.linkedin.com/in/rayankrishnan/) for designing the beautiful and engaging website.